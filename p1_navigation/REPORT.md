[//]: # (Image References)

[image2]: https://github.com/safdark/DRL_course_projects/blob/master/p1_navigation/docs/screenshot_bananan_bot_graph.png "Performance Graph"
[image3]: https://github.com/safdark/DRL_course_projects/blob/master/p1_navigation/docs/screenshot_q_learning_algorithm.png "Q-Learning"

# Project 1: Navigation
[![Click for video of trained agent][image1]](https://youtu.be/2LZEazw_taM)

## Table of Contents

- [Introduction](#introduction)
- [Implementation](#implementation)
	- [Driver](#driver)
	- [Q-Learning Agent](#q-learning-agent)
	- [Deep Q-Network](#deep-q-network)
		- [Network Architecture](#network-architecture)
- [Results](#results)
- [Future Enhancements](#future-enhancemets)
- [Appendix](#appendix)
	- [Learning from Pixels](#learning-from-pixels)

## Implementation

The following sections break down the implementation of the DQN agent into 3 components:
- **`The Driver`**: that provides the scaffolding for the environment and the agent to interact with each other.
- **`The Q-Learning Algorithm`**: that the agent uses in conjunction with a deep neural network.
- **`Deep Q-Network (DNN)`**: the function approximator(s) that is(are) utilized by the Q-learning algorithm used by the agent.

### Driver

For this project, the IPython Notebook serves as the driver. There are 2 notebooks:
- **`Navigation-Trainer.ipynb`**: This does the training and saves the optimized parameters to a file ('checkpoint.pth')
- **`Navigation-Visualizer.ipynb`**: This allows us to visualize the performance of the agent that is loaded from the checkpoint file generated by the trainer.

### Q-Learning Agent

The agent lies at the heart of the implements the Q-Learning algorithm by using the Deep Q-Network as a function approximator that gradually learns how to accurately predict the Q-value for every state+action combination, thereby allowing the agent to follow the action that offers the best expected reward.

The following optimizations have been used:
- **`Experience replay`**: A replay buffer has been implemented that stores the last 'BUFFER_SIZE' experience tuples, with each tuple holding the appropriately named values: <State, Action, Reward, Next-State, Done>. Experience replay allows the agent to train its Q-Network using raw cause->effect data that prevents the agent from deriving any implicit correlations between pieces of chronological data. This is analogous to the concept of shuffling that exploits the stochasticity of mini-batches that are randomly selected from the training data, for training regular deep neural networks.
- **`Fixed Q-Targets`**: 2 Q-Networks are used by the agent internally - the local Q-Network and the target Q-Network. Periodically, the parameters from the local Q-Network are copied over (with a dampening factor 'TAU') to the target Q-Network.

The final algorithm, incorporating the aforementioned optimizations into the vanilla Q-learning algorithm is as follows:
![Q-Learning][image3]

The following hyperparameter values have been used for this algorithm:
- **`BUFFER_SIZE`** : int(1e5) # replay buffer size
- **`GAMMA`**: 0.99            # discount factor
- **`UPDATE_EVERY`** : 40      # steps after which to copy over parameters to the fixed Q-target 
- **`TAU`** : 1e-3             # for soft update of target parameters

### Deep Q-Network

Deep Learning has found tremendous use in reinforcement learning, because with sufficient depth, such a network can approximate _*any*_ arbitrary function, according to the 'Universal Approximation Theorem'. The solution to a reinforcement learning problem too can be viewed as a multivariate function with multiple inputs and multiple outputs. Here, the inputs comprise the 'state' of the environment that is relevant to the agent and its actions; the outputs are the expected cumulative rewards that the agent can expect to receive (by the end of the game) for each possible action it takes from the given input state...i,e the Q-Value for the given state and each action.

Training the network requires letting the agent play a set of games, until its performance on the game is considered adequate. Various hyperparameters, as with any other deep learning solution, are required to be set for this to be achieved with the fewest possible iterations (episodes) of the game. These hyperparameters will be discussed in the sections to follow, as needed.

#### Network Architecture

Here is the architecture of the network that is used by the agent:
```
QNetwork(
  (fc1): Linear(in_features=37, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=4, bias=True)
)
```

The following hyperparameter values have been used:
- **`BATCH_SIZE`** : 128       # batch size
- **`LR`** : 5e-4              # learning rate 

## Results

There are 3 graphs here.

1. The first one shows the "Accuracy" with which the agent picks bananas. It is the ratio of the yellow bananas picked to the total bananas picked. In essence, it conveys how efficiently the agent picked the yellow bananas while _avoiding_ the blue bananas. Any value less than 1 just means that the agent picked up some blue bananas along the way. Obviously, the accuracy should be as close to 1 as possible, and, as seen in this graph below, the agent reaches quite close to that.
2. The second graph shows the actual total score per episode. Assuming a high accuracy score, this value also points to how efficiently the agent navigated the _open spaces_ to pick up as many bananas it could. In the graph below, the agent reaches a score in the vicinity of ~18.
3. The third graph is what this project is actually after. It shows the average score over the last 100 episodes, or all prior episodes, whichever is smaller. As you can see, the agent is able to finally settle on a score of ~18 over a 100-episode period, starting the ~2500th episode of training. However, it should be noted that the agent hit the 13+ mark around the 1500th episode!

![Performance Graph][image2]


## Future Enhancements

One of the drawbacks of the implementation above is that the agent took a long time (~1500 episodes) to reach the 100-episode average score of 13+. This can be improved in several ways. Here are a few such techniques.

1. Incorporating Double DQN Learning
2. Incorporating Duelling Q-Learning
3. Prioritized experience replay
4. Learning from Pixels (which is discussed at length in the appendix)

## Appendix

### Learning from Pixels

So far, the agent learned from information such as its velocity, along with ray-based perception of objects around its forward direction. A more challenging task would be to learn directly from pixels!

To solve this harder task, another Unity environment needs to be downloaded. This environment is almost identical to the previous environment, except that the state retrieved from the environment is a 84 x 84 RGB image, corresponding to the agent's first-person view.

You need only select the environment that matches your operating system:
- Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/VisualBanana_Linux.zip)
- Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/VisualBanana.app.zip)
- Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/VisualBanana_Windows_x86.zip)
- Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/VisualBanana_Windows_x86_64.zip)

Then, place the file in the `p1_navigation/` folder and unzip (or decompress) the file. Next, open `Navigation_Pixels.ipynb` and follow the instructions to learn how to use the Python API to control the agent.

(_For AWS_) If you'd like to train the agent on AWS, you must follow the instructions to [set up X Server](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md), and then download the environment for the **Linux** operating system above.
